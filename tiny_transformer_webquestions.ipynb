{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Lib"
      ],
      "metadata": {
        "id": "edkL6se0sOfV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkIEaD0iVhuK"
      },
      "outputs": [],
      "source": [
        "# Установим свежую версию TensorFlow для поддержки слоя `tf.keras.layers.MultiHeadAttention`.\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "!pip install protobuf~=3.20.3\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "oDKeXj9PscBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Фреймворк Tensorflow\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "\n",
        "# Расщепление выборки\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Токенизатор\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "\n",
        "# Линейная алгебра\n",
        "import numpy as np\n",
        "\n",
        "# Вывод графиков\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Регулярные выражения\n",
        "import re\n",
        "\n",
        "# Файловая система\n",
        "import pathlib\n",
        "\n",
        "# Отключим мешаюшие предупреждения\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "UJNrKBzPsbnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ],
      "metadata": {
        "id": "lrg1wXQitFKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples, metadata = tfds.load('web_questions', with_info=True, as_supervised=False)"
      ],
      "metadata": {
        "id": "_UMsziUbmNxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразуем датасет в список для удобного разделения\n",
        "train_examples = list(examples['train'])\n",
        "\n",
        "# Разделяем данные на обучающую и валидационную выборки (80% обучающая и 20% валидационная)\n",
        "train_examples, val_examples = train_test_split(train_examples, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "GCAbd5wzma0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Размер обучающей выборки: ', len(train_examples))\n",
        "print('Размер валидационной выборки: ', len(val_examples))"
      ],
      "metadata": {
        "id": "bFLCiOvGm4PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выводим несколько примеров из обучающей выборки\n",
        "for example in train_examples[:1]:\n",
        "    print('Пример:')\n",
        "    print('Вопрос:', example['question'].numpy().decode('utf-8'))\n",
        "    print('Ответ:', example['answers'].numpy())"
      ],
      "metadata": {
        "id": "srJ6oITRm6Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokinizer"
      ],
      "metadata": {
        "id": "wYz4EjvMn7VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 8000\n",
        "# Параметры токенизатора (lower_case - приводим к нижнему регистру)\n",
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "\n",
        "# Определяем токены, с которыми работает токенизатор\n",
        "# [START] - начало строки\n",
        "# [END]   - конец строки\n",
        "# [UNK]   - неизвестное слово\n",
        "# [PAD]   - используется для выравнивания длин всех предложений\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    # Желаемый размер словаря\n",
        "    vocab_size = VOCAB_SIZE,\n",
        "    # Токены включаемые в словарь\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    # Аргументы для `text.BertTokenizer`\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    # Аргументы для `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "    learn_params={}, # не используем, но определяем чтобы не было ошибок\n",
        ")"
      ],
      "metadata": {
        "id": "_MuXkSdenApN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_questions = [example['question'].numpy().decode('utf-8') for example in train_examples]"
      ],
      "metadata": {
        "id": "SgI0NFXK7dDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_vocab_args = dict(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    learn_params={},\n",
        ")"
      ],
      "metadata": {
        "id": "pCGdvEN87g1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем словарь для вопросов\n",
        "%%time\n",
        "question_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    tf.data.Dataset.from_tensor_slices(train_questions).batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "id": "CIHZ-O5l7kru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Словарь для вопросов:\")\n",
        "print(question_vocab[:10])\n",
        "print(question_vocab[100:110])\n",
        "print(question_vocab[1000:1010])\n",
        "print(question_vocab[-10:])"
      ],
      "metadata": {
        "id": "X9aSMCPa7r7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "metadata": {
        "id": "aktvW6ozEItv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохраняем словарь в файл\n",
        "write_vocab_file('question_vocab.txt', question_vocab)"
      ],
      "metadata": {
        "id": "d29JZ7oL7twI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ответы могут быть представлены списком байтовых строк, извлекаем первый ответ из списка\n",
        "train_answers = [' '.join([ans.decode('utf-8') for ans in example['answers'].numpy()]) for example in train_examples]"
      ],
      "metadata": {
        "id": "8GA2Zo-oEmIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем словарь для ответов\n",
        "%%time\n",
        "answer_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    tf.data.Dataset.from_tensor_slices(train_answers).batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "id": "jzwOO8rQEbpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Словарь для ответов:\")\n",
        "print(answer_vocab[:10])\n",
        "print(answer_vocab[100:110])\n",
        "print(answer_vocab[1000:1010])\n",
        "print(answer_vocab[-10:])"
      ],
      "metadata": {
        "id": "WqbX0WjGEfaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_vocab_file('answer_vocab.txt', answer_vocab)"
      ],
      "metadata": {
        "id": "lGn_PxNkEde5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls *.txt"
      ],
      "metadata": {
        "id": "xoNxTnZY7uRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download tokinizer"
      ],
      "metadata": {
        "id": "TMQ-lLnCQ_ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем токенизаторы на основе словарей вопросов и ответов\n",
        "question_tokenizer = text.BertTokenizer('question_vocab.txt', **bert_tokenizer_params)\n",
        "answer_tokenizer = text.BertTokenizer('answer_vocab.txt', **bert_tokenizer_params)"
      ],
      "metadata": {
        "id": "U9Ol7BflQ4ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Отберем строки для примера:')\n",
        "# Проходим по первым трем примерам из списка\n",
        "for example in train_examples[:3]:  # Отбираем три примера\n",
        "    answers = [ans.decode('utf-8') for ans in example['answers'].numpy()]  # Декодируем байтовые строки\n",
        "    print(answers)"
      ],
      "metadata": {
        "id": "bIfX7ffARBX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Отберем строки для примера:')\n",
        "# Отбираем вопросы и ответы для примера\n",
        "for example in train_examples[:3]:  # Отбираем первые 3 примера\n",
        "    question = example['question'].numpy().decode('utf-8')\n",
        "    answers = [ans.decode('utf-8') for ans in example['answers'].numpy()]\n",
        "\n",
        "print('Вопрос:', question)\n",
        "print('Ответы:', answers)"
      ],
      "metadata": {
        "id": "Ibb1cAg2SBBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизируем вопрос\n",
        "question_tokens = question_tokenizer.tokenize([question])\n",
        "question_tokens = question_tokens.merge_dims(-2, -1)  # Объединяем оси для создания последовательности токенов\n",
        "\n",
        "print('Токенизированный вопрос:', question_tokens.to_list())"
      ],
      "metadata": {
        "id": "rnfIS8AaUJbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизируем ответы\n",
        "answer_tokens = answer_tokenizer.tokenize([answers[0]])  # Токенизируем первый ответ\n",
        "answer_tokens = answer_tokens.merge_dims(-2, -1)\n",
        "\n",
        "print('Токенизированный ответ:', answer_tokens.to_list())"
      ],
      "metadata": {
        "id": "A9Bwsf3cUNe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обратное преобразование (детокенизация) для проверки\n",
        "detokenized_question = question_tokenizer.detokenize(question_tokens)\n",
        "detokenized_answer = answer_tokenizer.detokenize(answer_tokens)\n",
        "\n",
        "print('Детокенизированный вопрос:', tf.strings.reduce_join(detokenized_question, separator=' ', axis=-1).numpy())\n",
        "print('Детокенизированный ответ:', tf.strings.reduce_join(detokenized_answer, separator=' ', axis=-1).numpy())"
      ],
      "metadata": {
        "id": "lKsNI84dURap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Len"
      ],
      "metadata": {
        "id": "USP5I97jRF5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = []\n",
        "\n",
        "batch_size = 1024  # Определяем размер батча\n",
        "num_batches = len(train_examples) // batch_size  # Количество полных батчей\n",
        "\n",
        "# Цикл по батчам\n",
        "for i in range(num_batches + 1):  # Обрабатываем все батчи\n",
        "    batch = train_examples[i * batch_size : (i + 1) * batch_size]  # Формируем батч\n",
        "\n",
        "    # Извлекаем вопросы и ответы для текущего батча\n",
        "    questions = [ex['question'].numpy().decode('utf-8') for ex in batch]\n",
        "    answers = [' '.join([ans.numpy().decode('utf-8') for ans in ex['answers']]) for ex in batch]\n",
        "\n",
        "    # Токенизируем вопросы\n",
        "    question_tokens = question_tokenizer.tokenize(questions)\n",
        "    lengths.append(question_tokens.row_lengths())\n",
        "\n",
        "    # Токенизируем ответы\n",
        "    answer_tokens = answer_tokenizer.tokenize(answers)\n",
        "    lengths.append(answer_tokens.row_lengths())"
      ],
      "metadata": {
        "id": "ddqYJVc8XCAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Объединяем длины всех токенов\n",
        "all_lengths = np.concatenate(lengths)\n",
        "\n",
        "# Строим гистограмму распределения длин\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "\n",
        "# Максимальная длина токенов\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Максимальное количество токенов в примере: {max_length}');"
      ],
      "metadata": {
        "id": "POANTpAKX31y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS=80"
      ],
      "metadata": {
        "id": "Cdg40OvFYkzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# START END ADD"
      ],
      "metadata": {
        "id": "jSk63wWWRLbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Получаем индексы токенов [START] и [END]\n",
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "    count = ragged.bounding_shape()[0]  # Количество предложений в батче\n",
        "    starts = tf.fill([count, 1], START)  # Токены [START]\n",
        "    ends = tf.fill([count, 1], END)      # Токены [END]\n",
        "    return tf.concat([starts, ragged, ends], axis=1)  # Добавляем токены в начало и конец"
      ],
      "metadata": {
        "id": "EGH2AAc4dvT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example in train_examples[:3]:  # Пример обработки первых 3 примеров\n",
        "    # Извлекаем и токенизируем вопросы и ответы\n",
        "    question = example['question'].numpy().decode('utf-8')\n",
        "    answers = [ans.decode('utf-8') for ans in example['answers'].numpy()]\n",
        "\n",
        "    question_tokens = question_tokenizer.tokenize([question])\n",
        "    answer_tokens = answer_tokenizer.tokenize([answers[0]])\n",
        "\n",
        "    # Приводим к двумерной форме, если это необходимо\n",
        "    question_tokens = question_tokens.merge_dims(-2, -1)\n",
        "    answer_tokens = answer_tokens.merge_dims(-2, -1)\n",
        "\n",
        "    # Добавляем токены начала и конца\n",
        "    question_tokens_with_start_end = add_start_end(question_tokens)\n",
        "    answer_tokens_with_start_end = add_start_end(answer_tokens)\n",
        "\n",
        "    # Детокенизация\n",
        "    detokenized_question = question_tokenizer.detokenize(question_tokens_with_start_end)\n",
        "    detokenized_answer = answer_tokenizer.detokenize(answer_tokens_with_start_end)\n",
        "\n",
        "    # Объединяем токены в строку для примера\n",
        "    final_question = tf.strings.reduce_join(detokenized_question, separator=' ', axis=-1)\n",
        "    final_answer = tf.strings.reduce_join(detokenized_answer, separator=' ', axis=-1)\n",
        "\n",
        "    print(\"Пример вопроса с токенами [START] и [END]:\")\n",
        "    print(final_question.numpy())  # Выведет массив строк\n",
        "\n",
        "    print(\"Пример ответа с токенами [START] и [END]:\")\n",
        "    print(final_answer.numpy())  # Выведет массив строк"
      ],
      "metadata": {
        "id": "JsrMErYpFGmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleanup Text"
      ],
      "metadata": {
        "id": "EwocdfE5RP89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "  # Удаление токенов, кроме \"[UNK]\".\n",
        "  # Поиск зарезервированных токенов кроме [UNK]\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  # \"Плохие\" токены для регулярки объединяем знаком ИЛИ (|)\n",
        "  bad_token_re = \"|\".join(bad_tokens)\n",
        "\n",
        "  # Ищем в строке регулярку\n",
        "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "  # Отсеиваем из исходной строки все найденные включения \"плохих\" токенов\n",
        "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "  # Сцепление строк.\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "zlLFIIThFqGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Отберем строки для примера:\n",
        "for example in train_examples[:3]:  # Берем первые 3 примера\n",
        "    question = example['question'].numpy().decode('utf-8')\n",
        "    answers = [ans.decode('utf-8') for ans in example['answers'].numpy()]\n",
        "\n",
        "    print(question)  # Вывод вопроса\n",
        "    print(answers)   # Вывод ответов"
      ],
      "metadata": {
        "id": "d7wGJN6AJAi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Детокенизация вопросов и ответов\n",
        "detokenized_question = question_tokenizer.detokenize(question_tokens)\n",
        "detokenized_answer = answer_tokenizer.detokenize(answer_tokens)"
      ],
      "metadata": {
        "id": "bNbuwJ1YJCsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detokenized_question"
      ],
      "metadata": {
        "id": "zTQGXfAxOWIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleanup_text(reserved_tokens, detokenized_question).numpy()"
      ],
      "metadata": {
        "id": "X5WQY2IyOw1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Tokinizer"
      ],
      "metadata": {
        "id": "bXAuFst9RTSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer(tf.Module):\n",
        "  def __init__(self, reserved_tokens, vocab_path):\n",
        "    # Определяем токенизатор\n",
        "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "    # Зарезервированные токены\n",
        "    self._reserved_tokens = reserved_tokens\n",
        "    # Путь к файлу словаря\n",
        "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "    # Читаем из файла словарь и делим по строкам\n",
        "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        "\n",
        "    # Для экспорта класса необходимо создать так называемые сигнатуры,\n",
        "    # чтобы tensorflow понимал с какими данными он работает\n",
        "\n",
        "    # Сигнатура для tokenize (работает с пакетами строк).\n",
        "    self.tokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "\n",
        "    # Сигнатура для `detokenize` и `lookup`\n",
        "    # Могут работать как с `Tensors`, так и `RaggedTensors`\n",
        "    # с тензорами формы [batch, tokens]\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.detokenize.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    # Методы `get_*` не имеют аргументов\n",
        "    self.get_vocab_size.get_concrete_function()\n",
        "    self.get_vocab_path.get_concrete_function()\n",
        "    self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "    # После определения сигнатур можно определить и сами методы класса\n",
        "\n",
        "  @tf.function\n",
        "  def tokenize(self, strings):\n",
        "    enc = self.tokenizer.tokenize(strings)\n",
        "    # Объединяем оси `word` и `word-piece` (как в примере выше)\n",
        "    enc = enc.merge_dims(-2,-1)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc\n",
        "\n",
        "  @tf.function\n",
        "  def detokenize(self, tokenized):\n",
        "    words = self.tokenizer.detokenize(tokenized)\n",
        "    return cleanup_text(self._reserved_tokens, words) # очищаем перед выводом\n",
        "\n",
        "  @tf.function\n",
        "  def lookup(self, token_ids):\n",
        "    return tf.gather(self.vocab, token_ids) # возвращаем явное соответствие словаря токенам\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0] # определяем длину словаря по нулевому индексу формы\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_path(self):\n",
        "    return self._vocab_path # получение пути к файлу словаря\n",
        "\n",
        "  @tf.function\n",
        "  def get_reserved_tokens(self):\n",
        "    return tf.constant(self._reserved_tokens) # получение списка зарезервированных токенов\n"
      ],
      "metadata": {
        "id": "k8LO3elXQfne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание модуля TensorFlow с токенизаторами для вопросов и ответов\n",
        "tokenizers = tf.Module()\n",
        "tokenizers.question = CustomTokenizer(reserved_tokens, 'question_vocab.txt')\n",
        "tokenizers.answer = CustomTokenizer(reserved_tokens, 'answer_vocab.txt')"
      ],
      "metadata": {
        "id": "Ec8zwAh_SSiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение токенизаторов\n",
        "model_name = 'webquestions_converter'\n",
        "tf.saved_model.save(tokenizers, model_name)"
      ],
      "metadata": {
        "id": "Avqw69I8SZpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример токенизации вопросов\n",
        "encoded_questions = tokenizers.question.tokenize([example['question'].numpy().decode('utf-8') for example in train_examples[:3]])"
      ],
      "metadata": {
        "id": "fqsdAtg2Sepn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вывод токенизированных вопросов\n",
        "for row in encoded_questions.to_list():\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "Y4rIT9hTSiHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Детокенизация токенов вопросов\n",
        "round_trip_questions = tokenizers.question.detokenize(encoded_questions)\n",
        "for line in round_trip_questions.numpy():\n",
        "    print(line.decode('utf-8'))  # Выводим восстановленные вопросы"
      ],
      "metadata": {
        "id": "mplOulcZUXs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Поиск токенов для вопросов (возвращает текстовое представление токенов)\n",
        "tokens_questions = tokenizers.question.lookup(encoded_questions)\n",
        "tokens_questions"
      ],
      "metadata": {
        "id": "DRVE-aaLUZzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Dataset"
      ],
      "metadata": {
        "id": "c8fY6IVcml_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_batch(questions, answers):\n",
        "    # Токенизируем вопросы\n",
        "    questions = tokenizers.question.tokenize(questions)   # Токенизация вопросов\n",
        "    questions = questions[:, :MAX_TOKENS]                 # Обрезаем по длине MAX_TOKENS\n",
        "    questions = questions.to_tensor()                     # Преобразуем в тензор\n",
        "\n",
        "    # Токенизируем ответы\n",
        "    answers = tokenizers.answer.tokenize(answers)\n",
        "    answers = answers[:, :(MAX_TOKENS+1)]                 # Обрезаем до MAX_TOKENS+1 для учета [END]\n",
        "\n",
        "    # Разделение на вход декодера и метки\n",
        "    answer_inputs = answers[:, :-1].to_tensor()           # Убираем [END] для входа декодера\n",
        "    answer_labels = answers[:, 1:].to_tensor()            # Убираем [START] для меток\n",
        "\n",
        "    return (questions, answer_inputs), answer_labels\n"
      ],
      "metadata": {
        "id": "5FW4wVXaU7c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Размер буфера в памяти при подготовке датасета\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# Размер пакета\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "cPOyDsdgnHQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batches(ds):\n",
        "    return (\n",
        "        ds\n",
        "        .shuffle(BUFFER_SIZE)                     # Перемешиваем данные\n",
        "        .batch(BATCH_SIZE)                        # Делим датасет на пакеты\n",
        "        .map(prepare_batch, num_parallel_calls=tf.data.AUTOTUNE)  # Применяем функцию prepare_batch\n",
        "        .prefetch(buffer_size=tf.data.AUTOTUNE))  # Загружаем данные заранее"
      ],
      "metadata": {
        "id": "eTmYjlByo7V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразуем train_examples и val_examples в два списка: вопросов и ответов\n",
        "train_questions = [ex['question'].numpy().decode('utf-8') for ex in train_examples]\n",
        "train_answers = [ex['answers'][0].numpy().decode('utf-8') for ex in train_examples]\n",
        "\n",
        "val_questions = [ex['question'].numpy().decode('utf-8') for ex in val_examples]\n",
        "val_answers = [ex['answers'][0].numpy().decode('utf-8') for ex in val_examples]\n",
        "\n",
        "# Создаем датасеты для вопросов и ответов\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_questions, train_answers))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_questions, val_answers))\n",
        "\n",
        "# Корректная функция для пакетов с разбиением на вопросы и ответы\n",
        "def make_batches(ds):\n",
        "    return (\n",
        "        ds\n",
        "        .shuffle(BUFFER_SIZE)                     # Перемешиваем данные\n",
        "        .batch(BATCH_SIZE)                        # Делим датасет на пакеты\n",
        "        .map(lambda q, a: prepare_batch(q, a), num_parallel_calls=tf.data.AUTOTUNE)  # Применяем функцию prepare_batch\n",
        "        .prefetch(buffer_size=tf.data.AUTOTUNE))  # Загружаем данные заранее\n",
        "\n",
        "# Создание пакетов данных\n",
        "train_batches = make_batches(train_dataset)\n",
        "val_batches = make_batches(val_dataset)\n",
        "\n",
        "# Извлечение одного пакета данных для проверки\n",
        "for (questions, answers), answer_labels in train_batches.take(1):\n",
        "    break\n",
        "\n",
        "# Вывод размеров тензоров\n",
        "print(questions.shape)     # Размер вопросов\n",
        "print(answers.shape)       # Размер ответов\n",
        "print(answer_labels.shape) # Размер меток для ответов\n"
      ],
      "metadata": {
        "id": "FjfQCZVxpQFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# length - порядковый номер слова в фразе\n",
        "# depth - размер пространства эмбеддинга\n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # форма (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # форма (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # форма (1, depth)\n",
        "  angle_rads = positions * angle_rates      # форма (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)  # указываем тип возвращаемых данных"
      ],
      "metadata": {
        "id": "GlLki9kSvAEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вычислим позиционное кодирование для вопросов и ответов\n",
        "pos_encoding_questions = positional_encoding(length=2048, depth=512)\n",
        "pos_encoding_answers = positional_encoding(length=2048, depth=512)\n",
        "\n",
        "# Проверим форму\n",
        "print(\"Позиционное кодирование для вопросов:\", pos_encoding_questions.shape)\n",
        "print(\"Позиционное кодирование для ответов:\", pos_encoding_answers.shape)"
      ],
      "metadata": {
        "id": "aElRA9vuvjDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Embedding"
      ],
      "metadata": {
        "id": "Z-8YYpuD8Ejz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)  # Эмбеддинги токенов\n",
        "        self.pos_encoding = positional_encoding(length=MAX_TOKENS, depth=d_model)         # Позиционное кодирование\n",
        "\n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "        return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        length = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Масштабирование эмбеддингов\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # Добавление позиционного кодирования\n",
        "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "JzvLvhjPwRJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание слоя позиционного эмбеддинга для вопросов и ответов\n",
        "embed_question = PositionalEmbedding(vocab_size=tokenizers.question.get_vocab_size().numpy(), d_model=512)\n",
        "embed_answer = PositionalEmbedding(vocab_size=tokenizers.answer.get_vocab_size().numpy(), d_model=512)\n",
        "\n",
        "# Применение слоев эмбеддинга к токенизированным вопросам и ответам\n",
        "question_emb = embed_question(questions)  # Эмбеддинги для вопросов\n",
        "answer_emb = embed_answer(answers)        # Эмбеддинги для ответов"
      ],
      "metadata": {
        "id": "WKGVdE8nwklV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_emb._keras_mask"
      ],
      "metadata": {
        "id": "_RhlrTjQwoP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Attention"
      ],
      "metadata": {
        "id": "zYJd1qaCx70U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ],
      "metadata": {
        "id": "jnJHTFquwyTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CrossAttention"
      ],
      "metadata": {
        "id": "HcTFEH1fyjTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    # Пропускаем сигнал через многоголовое внимание\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,                        # запрос\n",
        "        key=context,                    # ключ\n",
        "        value=context,                  # значение\n",
        "        return_attention_scores=True)   # возвращаем оценки внимания\n",
        "\n",
        "    # Запоминаем оценки на будущее\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    # Добавляем остаточную связь и нормализацию\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "wCRby0k0yROr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# При инициализации через базовый класс передаем в слой tf.keras.layers.MultiHeadAttention\n",
        "# параметры num_heads - число голов, key_dim - размерность ключа\n",
        "sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(answer_emb.shape)\n",
        "print(question_emb.shape)\n",
        "print(sample_ca(answer_emb, question_emb).shape)"
      ],
      "metadata": {
        "id": "DEwN93skbNbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GlobalSelfAttention"
      ],
      "metadata": {
        "id": "Pe-rscOcynTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    # Пропускаем сигнал через многоголовое внимание\n",
        "    attn_output = self.mha(\n",
        "        query=x,  # запрос\n",
        "        value=x,  # ключ\n",
        "        key=x)    # значение\n",
        "\n",
        "    # Добавляем остаточную связь и нормализацию\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "1WHR29ZWyh6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(answer_emb.shape)\n",
        "print(sample_gsa(answer_emb).shape)"
      ],
      "metadata": {
        "id": "lWkEgvlHbmcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Casual Attention"
      ],
      "metadata": {
        "id": "YJhdPFe0YXrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)  # отличается от GlobalSelfAttention одним аргументом\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ticReWUMYZtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(question_emb.shape)\n",
        "print(sample_csa(question_emb).shape)"
      ],
      "metadata": {
        "id": "gIeDMJMIbqxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out1 = sample_csa(embed_answer(answers[:, :3]))\n",
        "out2 = sample_csa(embed_answer(answers))[:, :3]\n",
        "\n",
        "tf.reduce_max(abs(out1 - out2)).numpy()"
      ],
      "metadata": {
        "id": "DrzrnMK88IP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EzAvrdcW9vv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed Forward"
      ],
      "metadata": {
        "id": "35Ka-Ikg9wl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "PLcbtSaR8fQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_ffn = FeedForward(512, 2048)\n",
        "\n",
        "print(answer_emb.shape)\n",
        "print(sample_ffn(answer_emb).shape)"
      ],
      "metadata": {
        "id": "jK-rGE919zK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Layer"
      ],
      "metadata": {
        "id": "4jeB6INh-KMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,  # число голов\n",
        "        key_dim=d_model,      # размерность ключа\n",
        "        dropout=dropout_rate) # уровень регуляризации\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff) # число нейронов во втором и первом Dense слое, соответственно\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Hh9ovIB7-ACO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
        "\n",
        "print(question_emb.shape)\n",
        "print(sample_encoder_layer(question_emb).shape)"
      ],
      "metadata": {
        "id": "BN0SviQQ-Oni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # Инициируем переменные внутри класса\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Создаем объект класса позиционного кодирования\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    # Создаем объект класса для слоя кодировщика\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    # Создаем объект класса для слоя регуляризации\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # Форма x токена: (batch, seq_len)\n",
        "    # Прогоняем последовательность токенов через слой позиционного кодирования\n",
        "    x = self.pos_embedding(x)  # форма на выходе (batch_size, seq_len, d_model)\n",
        "\n",
        "    # Прогоняем последовательность токенов через слой регуляризации\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # Прогоняем последовательность токенов через num_layers слоев кодировщика\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # форма на выходе (batch_size, seq_len, d_model)"
      ],
      "metadata": {
        "id": "lYSidt6b_WGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Layer"
      ],
      "metadata": {
        "id": "vFkO4Wq4Acg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    # Слой внимания с причинно-следственной связью\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    # Слой с кросс-вниманием\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    # Слой прямого распространения\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # Пропускаем последовательность токенов через:\n",
        "    # Каузальный слой внимания\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    # Слой кросс-внимания и контекстным вектором из кодировщика\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Запомним оценки внимания на будущее\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "    # Через слой прямого распространения\n",
        "    x = self.ffn(x)  # Форма `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ],
      "metadata": {
        "id": "f4r-03W3AdT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "1AN5bdjzCiVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    # Инициируем переменные внутри класса\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Создаем объект класса позиционного кодирования\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    # Создаем объект класса для слоя регуляризации\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    # Создаем сразу стек слоев декодировщиков с помощью генератора списков по числу слоев\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    # Сбрасываем оценки внимания\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # Подаем на вход последовательность токенов x формой (batch, target_seq_len)\n",
        "\n",
        "    # Пропускаем через слой позиционного кодирования (и конечно же эмбеддинг)\n",
        "    x = self.pos_embedding(x)  # форма на выходе (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    # Регуляризация\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # Прогоняем через num_layers слоев декодировщиков\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    # Сохраняем оценки внимания из последнего слоя\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # Форма x на выходе (batch_size, target_seq_len, d_model)\n",
        "    return x"
      ],
      "metadata": {
        "id": "qclu-7DKAvG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "MabKuIG4CkQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    # Кодировщик\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "    # Декодировщик\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "    # Конечный слой\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Чтобы использовать метод `.fit` для обучения модели, необходимо передать\n",
        "    # все входные данные в первом аргументе\n",
        "    context, x  = inputs\n",
        "\n",
        "    # Передаем контекст в кодировщик\n",
        "    context = self.encoder(context)  # форма выходных данных (batch_size, context_len, d_model)\n",
        "\n",
        "    # Передаем контекст и целевой вектор в декодировщик\n",
        "    x = self.decoder(x, context)  # форма выходных данных (batch_size, target_len, d_model)\n",
        "\n",
        "    # Прогоняем выходные данные через финальный слой\n",
        "    logits = self.final_layer(x)  # форма выходных данных (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # После прохождения данных через все слои необходимо удалить\n",
        "      # маску, чтобы она не масштабировала, потери и метрики\n",
        "      # Обработчик ошибок позволяет избежать исключений при повторной попытке удаления\n",
        "      del logits._keras_mask\n",
        "    except AttributeError: # отлавливаем ошибку отсутствия аттрибута\n",
        "      pass\n",
        "\n",
        "    # Возвращаем наши логиты\n",
        "    return logits"
      ],
      "metadata": {
        "id": "ULV3T6XIAykS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "O7iYA0xCA4J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация модели трансформера для WebQuestions\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.question.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.answer.get_vocab_size().numpy(),\n",
        "    dropout_rate=dropout_rate)"
      ],
      "metadata": {
        "id": "XUC18x7qA7_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Входные данные: вопросы (input) и ответы (target)\n",
        "questions = tf.random.uniform((64, 29), maxval=tokenizers.question.get_vocab_size().numpy(), dtype=tf.int32)  # Пример вопросов\n",
        "answers = tf.random.uniform((64, 29), maxval=tokenizers.answer.get_vocab_size().numpy(), dtype=tf.int32)  # Пример ответов\n",
        "\n",
        "# Прогон через модель\n",
        "output = transformer((questions, answers))\n",
        "\n",
        "# Вывод формы входных данных и выходных данных модели\n",
        "print(questions.shape)  # Ожидаемая форма: (batch_size, seq_len)\n",
        "print(answers.shape)    # Ожидаемая форма: (batch_size, seq_len)\n",
        "print(output.shape)     # Ожидаемая форма: (batch_size, seq_len, target_vocab_size)\n",
        "\n",
        "# Вывод оценок внимания из последнего слоя декодера\n",
        "attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
        "print(attn_scores.shape)  # Ожидаемая форма: (batch_size, num_heads, target_seq_len, input_seq_len)"
      ],
      "metadata": {
        "id": "6ZYrXUj_CSSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer"
      ],
      "metadata": {
        "id": "b3vH6VPPC5C2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "gCvWBIDdB8Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "Bp_Wpj4ECwhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Количество батчей для обучения\n",
        "num_batches = 0\n",
        "for (batch, (_,_)) in enumerate(train_batches):\n",
        "  num_batches = batch\n",
        "print(num_batches)"
      ],
      "metadata": {
        "id": "_0WM8lWaC6ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(learning_rate(tf.range(num_batches*EPOCHS, dtype=tf.float32)))\n",
        "plt.ylabel('Скорость обучения')\n",
        "plt.xlabel('Шаг обучения')"
      ],
      "metadata": {
        "id": "IVLLLOvHC-ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(learning_rate(tf.range(num_batches*EPOCHS*50, dtype=tf.float32)))\n",
        "plt.ylabel('Скорость обучения')\n",
        "plt.xlabel('Шаг обучения')"
      ],
      "metadata": {
        "id": "XiQIC2rxDFom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция потерь с учетом маски\n",
        "def masked_loss(label, pred):\n",
        "  # Задаем маску, где метки не равны 0\n",
        "  mask = label != 0\n",
        "  # Определяем функцию потерь\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  # Важно чтобы mask и loss имели одинаковый тип данных\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  # Наложение маски на loss\n",
        "  loss *= mask\n",
        "\n",
        "  # Масштабирование потерь на маску\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "# Функция точности с учетом маски\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  # Оценка совпадения метки и предсказания\n",
        "  match = label == pred\n",
        "  # Задаем маску, где метки не равны 0\n",
        "  mask = label != 0\n",
        "\n",
        "  # Логическое И\n",
        "  match = match & mask\n",
        "\n",
        "  # Преобразуем к одному типу и масштабирование совпадений на маску\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "pa7c-_qlDmYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Компиляция модели с использованием настроек потерь, оптимизатора и метрики\n",
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])\n"
      ],
      "metadata": {
        "id": "85IQYL1LEEsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение модели на тренировочных данных с валидацией на валидационных данных\n",
        "transformer.fit(train_batches,\n",
        "                epochs=15,\n",
        "                validation_data=val_batches)"
      ],
      "metadata": {
        "id": "N18IUIUNEFGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "HkVbXPAoMkY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QABotTranslator(tf.Module):\n",
        "    def __init__(self, tokenizers, transformer):\n",
        "        self.tokenizers = tokenizers  # Токенизаторы для вопросов и ответов\n",
        "        self.transformer = transformer  # Модель трансформера\n",
        "\n",
        "    def __call__(self, question, max_length=MAX_TOKENS):\n",
        "        # Проверяем, что вопрос является тензором\n",
        "        assert isinstance(question, tf.Tensor)\n",
        "        if len(question.shape) == 0:\n",
        "            question = question[tf.newaxis]\n",
        "\n",
        "        # Токенизация вопроса\n",
        "        question = self.tokenizers.question.tokenize(question).to_tensor()\n",
        "        encoder_input = question\n",
        "\n",
        "        # Инициализация с токена [START] для генерации ответа\n",
        "        start_end = self.tokenizers.answer.tokenize([''])[0]\n",
        "        start = start_end[0][tf.newaxis]\n",
        "        end = start_end[1][tf.newaxis]\n",
        "\n",
        "        # Используем tf.TensorArray для динамического цикла генерации ответа\n",
        "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "        output_array = output_array.write(0, start)\n",
        "\n",
        "        for i in tf.range(max_length):\n",
        "            # На каждом шаге используем ранее сгенерированные токены для генерации следующего токена\n",
        "            output = tf.transpose(output_array.stack())\n",
        "            predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "            # Получаем последний предсказанный токен\n",
        "            predictions = predictions[:, -1:, :]\n",
        "            predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "            # Добавляем предсказанный токен к выходной последовательности\n",
        "            output_array = output_array.write(i + 1, predicted_id[0])\n",
        "\n",
        "            # Прерываем цикл, если сгенерирован токен [END]\n",
        "            if predicted_id == end:\n",
        "                break\n",
        "\n",
        "        # Преобразование сгенерированных токенов в текст\n",
        "        output = tf.transpose(output_array.stack())\n",
        "        text = self.tokenizers.answer.detokenize(output)[0]\n",
        "\n",
        "        # Преобразуем токены в человекочитаемый формат\n",
        "        tokens = self.tokenizers.answer.lookup(output)[0]\n",
        "\n",
        "        # Вычисляем внимание на последнем шаге\n",
        "        self.transformer([encoder_input, output[:, :-1]], training=False)\n",
        "        attention_weights = self.transformer.decoder.last_attn_scores\n",
        "\n",
        "        return text, tokens, attention_weights"
      ],
      "metadata": {
        "id": "it9q3JGwLEl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример использования чат-бота для вопрос-ответ с WebQuestions\n",
        "qa_translator = QABotTranslator(tokenizers=tokenizers, transformer=transformer)"
      ],
      "metadata": {
        "id": "nNn1ahzwNbDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(question):\n",
        "    \"\"\"\n",
        "    Функция принимает вопрос в виде строки и возвращает ответ, сгенерированный моделью.\n",
        "    \"\"\"\n",
        "    # Преобразуем вопрос в тензор\n",
        "    question_tensor = tf.constant(question)\n",
        "\n",
        "    # Получаем предсказанный ответ\n",
        "    translated_text, translated_tokens, attention_weights = qa_translator(question_tensor)\n",
        "\n",
        "    # Возвращаем ответ\n",
        "    return translated_text.numpy().decode('utf-8')\n",
        "\n",
        "# Пример вызова функции\n",
        "question = \"Who is the president of the USA?\"\n",
        "answer = get_answer(question)\n",
        "\n",
        "print(f\"Вопрос: {question}\")\n",
        "print(f\"Ответ: {answer}\")\n"
      ],
      "metadata": {
        "id": "GuKfLFgMMxA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"where is angola located?\"\n",
        "answer = get_answer(question)\n",
        "\n",
        "print(f\"Вопрос: {question}\")\n",
        "print(f\"Ответ: {answer}\")"
      ],
      "metadata": {
        "id": "rR1ehYQmNeUz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}